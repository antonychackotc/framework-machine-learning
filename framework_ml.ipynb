{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmDH5nv8m7opNlAKqhli9Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antonychackotc/framework-machine-learning/blob/main/framework_ml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxbHpmQWINz0"
      },
      "outputs": [],
      "source": [
        "# create a streamlit application based\n",
        "\n",
        "# upload a mulitple files and allow multiple format eg:- csv jason xcell etc\n",
        "# and analyse file\n",
        "\n",
        "# suggest me the which column is best for features colum and target column\n",
        "# if that file have no target column (unlabeled means) display unsupervised learning\n",
        "\n",
        "# if unsupervised learning\n",
        "# recomend which algorithm sute for this file\n",
        "# eg: kmeans clustering and explain why, or dbscan clustering and explain why\n",
        "# and manual select option also give to select feature column and algorithm a\n",
        "# and show time of loading time and alow\n",
        "\n",
        "# else: supervised learning\n",
        "# that dataset shows supervised learning\n",
        "# recommend which algorithm sute for this file\n",
        "# show classification and regression\n",
        "# show continous and descret\n",
        "# if regression\n",
        "# if continous\n",
        "# use - linear regression,polumomial regression,ridge regression and lasso regression, svm regression\n",
        "# if discret\n",
        "# use - decession tree regression, random forest regression, boosting regression,navebayes,knn regression, svm regression\n",
        "# and\n",
        "# if classify\n",
        "# display - binary or multiclass\n",
        "# if binary use - logistic regression, svm classifier\n",
        "# else : multiclass use - decession tree classifier, random forest classifier, boosting classifier, knn classifier, svm classifier\n",
        "\n",
        "# show recomendation of suitable algorithm - all algorithm manual select option - show accuracy with time\n",
        "\n",
        "# show metrix\n",
        "\n",
        "# show balanced or unbalanced dataset\n",
        "\n",
        "# show precession, recall, f1 score, accuracy show both balanced or unbalanced dataset\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app1.py"
      ],
      "metadata": {
        "id": "uHgwLcDAQjLb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "532a948a-7ac3-4966-9782-7fd6893aaca0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app1.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit\n",
        "!pip install -q pyngrok\n",
        "!pip install -q localtunnel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBZeLwFSQlpD",
        "outputId": "1f0a701e-2dc1-4094-8298-84b611d75c9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Could not find a version that satisfies the requirement localtunnel (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for localtunnel\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Machine learning**"
      ],
      "metadata": {
        "id": "SQAsrL1RlIsA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app1.py\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor\n",
        "from sklearn.svm import SVC, SVR\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
        "from xgboost import XGBClassifier, XGBRegressor\n",
        "from lightgbm import LGBMClassifier, LGBMRegressor\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, mean_squared_error, confusion_matrix)\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from io import StringIO\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_selection import RFE\n",
        "import time\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import io\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Application title\n",
        "st.title(\"Machine Learning\")\n",
        "st.subheader(\"File Analysis and Algorithm Recommendation\")\n",
        "st.text(\"try to upload file without outliers\")\n",
        "\n",
        "# Tabs for application workflow\n",
        "\n",
        "tabs = st.tabs([\n",
        "    \"Upload Files\",\n",
        "    \"Dataset Analysis\",\n",
        "    \"Supervised Learning\",\n",
        "    \"Download Train and Test Datasets with Results\",\n",
        "    \"Future Prediction\",\n",
        "    \"Unsupervised Learning\",\n",
        "    \"Data Quality Insights\"\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "# Tab 1: File Upload\n",
        "with tabs[0]:\n",
        "    st.header(\"Upload Your Files\")\n",
        "    uploaded_files = st.file_uploader(\n",
        "        \"Upload multiple files (CSV, Excel, JSON)\",\n",
        "        type=[\"csv\", \"xlsx\", \"json\"],\n",
        "        accept_multiple_files=True\n",
        "    )\n",
        "    datasets = {}\n",
        "    if uploaded_files:\n",
        "        for uploaded_file in uploaded_files:\n",
        "            file_type = uploaded_file.name.split(\".\")[-1].lower()\n",
        "            try:\n",
        "                if file_type == \"csv\":\n",
        "                    data = pd.read_csv(uploaded_file)\n",
        "                elif file_type in [\"xls\", \"xlsx\"]:\n",
        "                    data = pd.read_excel(uploaded_file)\n",
        "                elif file_type == \"json\":\n",
        "                    data = pd.read_json(uploaded_file)\n",
        "                else:\n",
        "                    st.error(f\"Unsupported file format: {file_type}\")\n",
        "                    continue\n",
        "\n",
        "                datasets[uploaded_file.name] = data\n",
        "                st.write(f\"Preview of **{uploaded_file.name}**\")\n",
        "                st.write(data.head())\n",
        "\n",
        "            except Exception as e:\n",
        "                st.error(f\"Error processing {uploaded_file.name}: {str(e)}\")\n",
        "\n",
        "# Dataset selection\n",
        "selected_dataset = st.sidebar.selectbox(\n",
        "    \"Select a dataset to analyze\",\n",
        "    options=list(datasets.keys()) if datasets else [],\n",
        "    help=\"Choose the dataset you uploaded for further analysis.\"\n",
        ")\n",
        "\n",
        "if selected_dataset:\n",
        "    data = datasets[selected_dataset]\n",
        "\n",
        "# Tab 2: Dataset Analysis\n",
        "with tabs[1]:\n",
        "    if selected_dataset:\n",
        "        st.header(f\"Analysis for {selected_dataset}\")\n",
        "        st.write(\"Shape of dataset:\", data.shape)\n",
        "\n",
        "        # Identify numeric and categorical columns\n",
        "        numeric_cols = data.select_dtypes(include=[\"number\"]).columns.tolist()\n",
        "        categorical_cols = data.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
        "        st.write(\"Numeric Columns:\", numeric_cols)\n",
        "        st.write(\"Categorical Columns:\", categorical_cols)\n",
        "\n",
        "        # Missing values\n",
        "        missing_values = data.isnull().sum()\n",
        "        st.write(\"Missing Values:\")\n",
        "        st.write(missing_values[missing_values > 0])\n",
        "\n",
        "        # Outliers detection\n",
        "        st.subheader(\"Outlier Detection (based on numeric columns):\")\n",
        "        for col in numeric_cols:\n",
        "            q1 = data[col].quantile(0.25)\n",
        "            q3 = data[col].quantile(0.75)\n",
        "            iqr = q3 - q1\n",
        "            outliers = data[(data[col] < (q1 - 1.5 * iqr)) | (data[col] > (q3 + 1.5 * iqr))]\n",
        "            st.write(f\"{col}: {len(outliers)} outliers\")\n",
        "\n",
        "        # Encoding recommendations\n",
        "        if categorical_cols:\n",
        "            st.subheader(\"Columns requiring encoding:\")\n",
        "            for col in categorical_cols:\n",
        "                unique_values = data[col].nunique()\n",
        "                encoding_type = \"Ordinal\" if unique_values < 10 else \"Nominal\"\n",
        "                st.write(f\"{col}: {encoding_type} encoding\")\n",
        "\n",
        "        # Scaling recommendations\n",
        "        st.subheader(\"Columns requiring feature scaling (numeric only):\")\n",
        "        for col in numeric_cols:\n",
        "            if data[col].max() - data[col].min() > 1000:\n",
        "                st.write(col)\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    mean_squared_error,\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        ")\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# Tab 3: Supervised Learning\n",
        "with tabs[2]:\n",
        "    # Ensure dataset is loaded\n",
        "    if selected_dataset:\n",
        "        st.header(\"Supervised Learning Recommendations\")\n",
        "\n",
        "        # Target column selection\n",
        "        target_col = st.selectbox(\"Select the target column:\", [None] + list(data.columns))\n",
        "        if target_col:\n",
        "            y = data[target_col]\n",
        "            unique_values = y.nunique()\n",
        "\n",
        "            # Determine if the target is continuous or discrete\n",
        "            if y.dtype in [np.int64, np.float64]:\n",
        "                if unique_values > 20:\n",
        "                    inferred_task_type = \"Regression\"\n",
        "                    target_status = \"Continuous\"\n",
        "                else:\n",
        "                    inferred_task_type = \"Classification\"\n",
        "                    target_status = \"Discrete\"\n",
        "            else:\n",
        "                inferred_task_type = \"Classification\"\n",
        "                target_status = \"Discrete\"\n",
        "\n",
        "            # Display inferred target properties\n",
        "            st.success(f\"**Target Column Data Type:** {target_status}\")\n",
        "            st.success(f\"**Inferred Task Type:** {inferred_task_type}\")\n",
        "\n",
        "            # Check for balance in classification tasks\n",
        "            if inferred_task_type == \"Classification\":\n",
        "                class_distribution = y.value_counts(normalize=True)\n",
        "                max_class_proportion = class_distribution.max()\n",
        "                balance_status = \"Balanced\" if max_class_proportion <= 0.7 else \"Unbalanced\"\n",
        "                st.info(f\"**Dataset Balance Status:** {balance_status}\")\n",
        "                st.write(\"**Class Distribution:\")\n",
        "                st.bar_chart(class_distribution)\n",
        "\n",
        "                st.info(\n",
        "    \"Oversampling: - Add Extra Data\\n\"\n",
        "    \"1) Adds new samples of the minority class\\n\"\n",
        "    \"2) Used when there is not enough data\\n\"\n",
        "    \"3) Can be effective in small datasets\\n\"\n",
        "    \"4) Can lead to overfitting\\n\"\n",
        "    \"5) A popular technique is SMOTE (Synthetic Minority Over-sampling Technique)\"\n",
        "\n",
        "                )\n",
        "                st.warning(\"Oversampling:\\n\"\n",
        "                \"Oversampling can create artificial class distributions that are not representative of the real world\"\n",
        "                )\n",
        "                st.info(\n",
        "    \"Undersampling: - Remove Extra Data\\n\"\n",
        "    \"1)  Adds new samples of the majority class\\n\"\n",
        "    \"2) Used when there is too much data\\n\"\n",
        "    \"3) Can be effective in large datasets\\n\"\n",
        "    \"4) Can lead to loss of information\\n\"\n",
        "    \"5) Common methods include cluster centroids and Tomek links\"\n",
        ")\n",
        "                st.warning(\"Undersampling:\\n\"\n",
        "                \"Undersampling can lead to biased models because it can cause loss of information. \"\n",
        "                )\n",
        "\n",
        "\n",
        "\n",
        "            # Manual override for task type\n",
        "            manual_task_type = st.radio(\n",
        "                \"Select the problem type manually:\",\n",
        "                options=[\"Classification\", \"Regression\"],\n",
        "                index=0 if inferred_task_type == \"Classification\" else 1,\n",
        "            )\n",
        "            manual_data_type = st.radio(\n",
        "                \"Select the target type manually:\",\n",
        "                options=[\"Continuous\", \"Discrete\"],\n",
        "                index=0 if target_status == \"Continuous\" else 1,\n",
        "            )\n",
        "\n",
        "            # Display suggested algorithms based on manual selection\n",
        "            if manual_task_type == \"Regression\":\n",
        "                st.write(\"**You selected Regression.**\")\n",
        "                st.write(\"**Suggested Regression Algorithms:**\")\n",
        "                if manual_data_type == \"Continuous\":\n",
        "                    st.write(\"- Linear Regression\")\n",
        "                    st.write(\"- Polynomial Regression\")\n",
        "                    st.write(\"- Ridge Regression\")\n",
        "                    st.write(\"- Lasso Regression\")\n",
        "                    st.write(\"- SVM Regressor\")\n",
        "                else:\n",
        "                    st.write(\"- Decision Tree Regressor\")\n",
        "                    st.write(\"- Random Forest Regressor\")\n",
        "                    st.write(\"- Boosting Regressor (e.g., XGBoost)\")\n",
        "                    st.write(\"- KNN Regressor\")\n",
        "                    st.write(\"- SVM Regressor\")\n",
        "\n",
        "            elif manual_task_type == \"Classification\":\n",
        "                st.write(\"**You selected Classification.**\")\n",
        "                if unique_values == 2:\n",
        "                    st.success(\"**Suggested Binary Classification Algorithms:**\")\n",
        "                    st.write(\"- Logistic Regression\")\n",
        "                    st.write(\"- SVM Classifier\")\n",
        "                else:\n",
        "                    st.success(\"**Suggested Multiclass Classification Algorithms:**\")\n",
        "                    st.write(\"- Decision Tree Classifier\")\n",
        "                    st.write(\"- Random Forest Classifier\")\n",
        "                    st.write(\"- Boosting Classifier (e.g., XGBoost)\")\n",
        "                    st.write(\"- KNN Classifier\")\n",
        "                    st.write(\"- Naive Bayes Classifier\")\n",
        "\n",
        "            # Feature selection\n",
        "            features = st.multiselect(\"Select feature columns\", options=[col for col in data.columns if col != target_col])\n",
        "            if features:\n",
        "                X = data[features]\n",
        "                y = data[target_col]\n",
        "\n",
        "                # Map binary target column for classification\n",
        "                if manual_task_type == \"Classification\" and unique_values == 2:\n",
        "                    y = y.map({y.unique()[0]: 0, y.unique()[1]: 1})\n",
        "\n",
        "                # Train-test split\n",
        "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "                # Algorithm selection\n",
        "                algorithm = st.selectbox(\n",
        "                    \"Select Algorithm\",\n",
        "                    [\n",
        "                        \"Linear Regression\", \"Polynomial Regression\", \"Ridge Regression\", \"Lasso Regression\",\n",
        "                        \"ElasticNet Regression\", \"SVM Regression\", \"Decision Tree Regressor\",\n",
        "                        \"Random Forest Regressor\", \"Boosting Regressor\", \"KNN Regressor\"\n",
        "                    ] if manual_task_type == 'Regression' else [\n",
        "                        \"Logistic Regression\", \"SVM Classifier\", \"Decision Tree Classifier\",\n",
        "                        \"Random Forest Classifier\", \"Boosting Classifier\", \"KNN Classifier\",\n",
        "                        \"Naive Bayes Classifier\"\n",
        "                    ]\n",
        "                )\n",
        "\n",
        "                # Initialize model\n",
        "                model = None\n",
        "                if algorithm == \"Linear Regression\":\n",
        "                    model = LinearRegression()\n",
        "                elif algorithm == \"Polynomial Regression\":\n",
        "                    model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())\n",
        "                elif algorithm == \"Ridge Regression\":\n",
        "                    model = Ridge()\n",
        "                elif algorithm == \"Lasso Regression\":\n",
        "                    model = Lasso()\n",
        "                elif algorithm == \"Decision Tree Regressor\":\n",
        "                    model = DecisionTreeRegressor()\n",
        "                elif algorithm == \"Random Forest Regressor\":\n",
        "                    model = RandomForestRegressor()\n",
        "                elif algorithm == \"Boosting Regressor\":\n",
        "                    model = XGBRegressor()\n",
        "                elif algorithm == \"KNN Regressor\":\n",
        "                    model = KNeighborsRegressor()\n",
        "                elif algorithm == \"Logistic Regression\":\n",
        "                    model = LogisticRegression()\n",
        "                elif algorithm == \"SVM Classifier\":\n",
        "                    model = SVC()\n",
        "                elif algorithm == \"Decision Tree Classifier\":\n",
        "                    model = DecisionTreeClassifier()\n",
        "                elif algorithm == \"Random Forest Classifier\":\n",
        "                    model = RandomForestClassifier()\n",
        "                elif algorithm == \"Boosting Classifier\":\n",
        "                    model = XGBClassifier()\n",
        "                elif algorithm == \"KNN Classifier\":\n",
        "                    model = KNeighborsClassifier()\n",
        "                elif algorithm == \"Naive Bayes Classifier\":\n",
        "                    model = GaussianNB()\n",
        "\n",
        "                # Train and evaluate the model\n",
        "                start_time = time.time()\n",
        "                model.fit(X_train, y_train)\n",
        "                y_pred = model.predict(X_test)\n",
        "                end_time = time.time()\n",
        "\n",
        "                st.write(f\"Training Time: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "                if manual_task_type == \"Classification\":\n",
        "                    st.write(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "                    st.write(f\"Precision: {precision_score(y_test, y_pred, average='macro'):.4f}\")\n",
        "                    st.write(f\"Recall: {recall_score(y_test, y_pred, average='macro'):.4f}\")\n",
        "                    st.write(f\"F1 Score: {f1_score(y_test, y_pred, average='macro'):.4f}\")\n",
        "\n",
        "                    st.info(\"Precision = TP/(TP + FP)\")\n",
        "                    st.info(\"Recall = TP/(TP + FN)\")\n",
        "\n",
        "                    # Display classification report as a table\n",
        "                    report = classification_report(y_test, y_pred, output_dict=True)\n",
        "                    report_df = pd.DataFrame(report).transpose()\n",
        "                    st.table(report_df.style.format(precision=4))\n",
        "\n",
        "                    # Plot Confusion Matrix\n",
        "                    st.subheader(\"Confusion Matrix\")\n",
        "                    cm = confusion_matrix(y_test, y_pred)\n",
        "                    fig, ax = plt.subplots()\n",
        "\n",
        "                    # Annotate the confusion matrix with additional labels\n",
        "                    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Predicted 0\", \"Predicted 1\"], yticklabels=[\"Actual 0\", \"Actual 1\"])\n",
        "                    ax.set_xlabel(\"Predicted Labels\")\n",
        "                    ax.set_ylabel(\"True Labels\")\n",
        "                    ax.set_title(\"Confusion Matrix with Labels\")\n",
        "\n",
        "                    # Extract TP, TN, FP, FN values\n",
        "                    tn, fp, fn, tp = cm.ravel()\n",
        "                    st.write(f\"**True Positive (TP): {tp}** - Predicted 1 and Actual 1\")\n",
        "                    st.write(f\"**True Negative (TN): {tn}** - Predicted 0 and Actual 0\")\n",
        "                    st.write(f\"**False Positive (FP): {fp}** - Predicted 1 but Actual 0\")\n",
        "                    st.write(f\"**False Negative (FN): {fn}** - Predicted 0 but Actual 1\")\n",
        "\n",
        "                    # Display the heatmap\n",
        "                    st.pyplot(fig)\n",
        "\n",
        "                    st.markdown(\n",
        "    \"\"\"\n",
        "    <b>True Positive</b>: The test correctly predicts a positive outcome when the actual outcome is positive.<br>\n",
        "    <b>True Negative</b>: The test correctly predicts a negative outcome when the actual outcome is negative.<br>\n",
        "    <b>False Positive</b>: The test incorrectly predicts a positive outcome when the actual outcome is negative.<br>\n",
        "    <b>False Negative</b>: The test incorrectly predicts a negative outcome when the actual outcome is positive.\n",
        "    \"\"\",\n",
        "    unsafe_allow_html=True\n",
        ")\n",
        "\n",
        "                # Check if the task is classification\n",
        "                    if manual_task_type == \"Classification\":\n",
        "                        # Add a column to indicate TP, TN, FP, FN\n",
        "                        result_labels = []\n",
        "                        for true, pred in zip(y_test, y_pred):\n",
        "                            if true == 1 and pred == 1:\n",
        "                                result_labels.append(\"True Positive\")\n",
        "                            elif true == 0 and pred == 0:\n",
        "                                result_labels.append(\"True Negative\")\n",
        "                            elif true == 0 and pred == 1:\n",
        "                                result_labels.append(\"False Positive\")\n",
        "                            elif true == 1 and pred == 0:\n",
        "                                result_labels.append(\"False Negative\")\n",
        "\n",
        "                        # Add this result to the test data for download\n",
        "                        test_data_with_results = X_test.copy()\n",
        "                        test_data_with_results[target_col] = y_test\n",
        "                        test_data_with_results[\"Predicted\"] = y_pred\n",
        "                        test_data_with_results[\"Result\"] = result_labels\n",
        "\n",
        "                        # Check the shape of the data\n",
        "                        print(test_data_with_results.shape)\n",
        "                        print(test_data_with_results.head())\n",
        "\n",
        "                        # Provide a download link\n",
        "                        st.subheader(\"Download Updated Dataset\")\n",
        "                        buffer = io.BytesIO()\n",
        "                        test_data_with_results.to_csv(buffer, index=False)\n",
        "                        buffer.seek(0)\n",
        "                        buffer.flush()  # Ensure everything is written to the buffer\n",
        "\n",
        "                        st.write(\"Shape of dataset:\", data.shape)\n",
        "                        st.success(\"downloaded data is test data\")\n",
        "                        st.download_button(\n",
        "                            label=\"Download Dataset with Results (CSV)\",\n",
        "                            data=buffer,\n",
        "                            file_name=\"updated_dataset_with_results.csv\",\n",
        "                            mime=\"text/csv\",\n",
        "                        )\n",
        "\n",
        "\n",
        "                elif manual_task_type == \"Regression\":\n",
        "                    mse = mean_squared_error(y_test, y_pred)\n",
        "                    st.write(f\"Mean Squared Error: {mse:.4f}\")\n",
        "\n",
        "                    # Calculate R² score\n",
        "                    r2 = r2_score(y_test, y_pred)\n",
        "                    st.write(f\"R² Score: {r2:.4f}\")\n",
        "\n",
        "                    st.write(\"MSE for Regression Tasks\")\n",
        "                    st.write(\"The range of values for mean squared error (MSE) is zero to infinity (\\(0\\le \\text{MSE}<\\infty \\)). A lower MSE value indicates a better model. \")\n",
        "                    st.write(\"The square root of MSE is called root mean square error (RMSE)\")\n",
        "                    st.write(\"the Mean Square Error (MSE) is a crucial metric for evaluating the performance of predictive models. It measures the average squared difference between the predicted and the actual target values within a dataset.\")\n",
        "        else:\n",
        "            st.warning(\"Please select a target column to proceed.\")\n",
        "\n",
        "# Tab 4: Download Datasets with Results\n",
        "with tabs[3]:\n",
        "    st.header(\"Download Train and Test Datasets with Results\")\n",
        "\n",
        "\n",
        "    # Ensure features and target column are selected\n",
        "    if features and target_col:\n",
        "        if manual_task_type == \"Classification\":\n",
        "            # Generate results for the train dataset\n",
        "            train_result_labels = []\n",
        "            for true, pred in zip(y_train, model.predict(X_train)):\n",
        "                if true == 1 and pred == 1:\n",
        "                    train_result_labels.append(\"True Positive\")\n",
        "                elif true == 0 and pred == 0:\n",
        "                    train_result_labels.append(\"True Negative\")\n",
        "                elif true == 0 and pred == 1:\n",
        "                    train_result_labels.append(\"False Positive\")\n",
        "                elif true == 1 and pred == 0:\n",
        "                    train_result_labels.append(\"False Negative\")\n",
        "\n",
        "            train_data_with_results = X_train.copy()\n",
        "            train_data_with_results[target_col] = y_train.values\n",
        "            train_data_with_results[\"Predicted\"] = model.predict(X_train)\n",
        "            train_data_with_results[\"Result\"] = train_result_labels\n",
        "\n",
        "            # Generate results for the test dataset\n",
        "            test_result_labels = []\n",
        "            for true, pred in zip(y_test, y_pred):\n",
        "                if true == 1 and pred == 1:\n",
        "                    test_result_labels.append(\"True Positive\")\n",
        "                elif true == 0 and pred == 0:\n",
        "                    test_result_labels.append(\"True Negative\")\n",
        "                elif true == 0 and pred == 1:\n",
        "                    test_result_labels.append(\"False Positive\")\n",
        "                elif true == 1 and pred == 0:\n",
        "                    test_result_labels.append(\"False Negative\")\n",
        "\n",
        "            test_data_with_results = X_test.copy()\n",
        "            test_data_with_results[target_col] = y_test.values\n",
        "            test_data_with_results[\"Predicted\"] = y_pred\n",
        "            test_data_with_results[\"Result\"] = test_result_labels\n",
        "\n",
        "            # Provide download links for train and test datasets\n",
        "            st.subheader(\"Download Train Dataset with Results\")\n",
        "            train_buffer = io.BytesIO()\n",
        "            train_data_with_results.to_csv(train_buffer, index=False)\n",
        "            train_buffer.seek(0)\n",
        "            st.download_button(\n",
        "                label=\"Download Train Dataset with Results (CSV)\",\n",
        "                data=train_buffer,\n",
        "                file_name=\"train_dataset_with_results.csv\",\n",
        "                mime=\"text/csv\",\n",
        "            )\n",
        "\n",
        "            st.subheader(\"Download Test Dataset with Results\")\n",
        "            test_buffer = io.BytesIO()\n",
        "            test_data_with_results.to_csv(test_buffer, index=False)\n",
        "            test_buffer.seek(0)\n",
        "            st.download_button(\n",
        "                label=\"Download Test Dataset with Results (CSV)\",\n",
        "                data=test_buffer,\n",
        "                file_name=\"test_dataset_with_results.csv\",\n",
        "                mime=\"text/csv\",\n",
        "            )\n",
        "\n",
        "        elif manual_task_type == \"Regression\":\n",
        "            st.warning(\"This feature is only available for Classification tasks.\")\n",
        "    else:\n",
        "        st.warning(\"Please select features and a target column in the previous tab to proceed.\")\n",
        "\n",
        "\n",
        "#future prediction\n",
        "with tabs[4]:\n",
        "    if selected_dataset:\n",
        "        st.header(\"Future Prediction\")\n",
        "\n",
        "        # Ensure model is trained and features are available\n",
        "        if 'model' in locals() and model is not None:\n",
        "            st.write(\"**Please input values for the following features to make a prediction:**\")\n",
        "\n",
        "            # Get feature names (assuming the model has been trained with 'X')\n",
        "            model_features = X.columns\n",
        "\n",
        "            # Create a dictionary to hold the user input\n",
        "            user_input = {}\n",
        "\n",
        "            # Create input fields for each feature\n",
        "            for feature in model_features:\n",
        "                user_input[feature] = st.text_input(f\"Enter value for {feature}:\")\n",
        "\n",
        "            # When the user presses the button to make a prediction\n",
        "            if st.button(\"Make Prediction\"):\n",
        "                try:\n",
        "                    # Check if all the inputs are filled and valid\n",
        "                    input_values = []\n",
        "                    for feature in model_features:\n",
        "                        value = user_input[feature]\n",
        "                        # Convert to float and append to input_values\n",
        "                        input_values.append(float(value))  # Could raise ValueError if the input is not a valid number\n",
        "\n",
        "                    # Ensure the input is in the correct shape (1, n_features)\n",
        "                    input_values = np.array(input_values).reshape(1, -1)\n",
        "\n",
        "                    # Make the prediction using the trained model\n",
        "                    prediction = model.predict(input_values)\n",
        "\n",
        "                    # Show the prediction result\n",
        "                    st.write(f\"**Prediction Result:** {prediction[0]}\")\n",
        "\n",
        "                except ValueError:\n",
        "                    st.error(\"Please enter valid numeric values for all features.\")\n",
        "                except Exception as e:\n",
        "                    st.error(f\"An error occurred: {str(e)}\")\n",
        "        else:\n",
        "            st.warning(\"Model is not trained yet. Please train the model first.\")\n",
        "\n",
        "\n",
        "\n",
        "# Tab 4: Unsupervised Learning\n",
        "with tabs[5]:\n",
        "    if selected_dataset:\n",
        "        st.header(\"Unsupervised Learning Recommendations\")\n",
        "\n",
        "        target_col = st.selectbox(\"Do you have a target column?\", options=[\"No Target (Unlabeled)\", \"Yes\"], index=0)\n",
        "\n",
        "        if target_col == \"No Target (Unlabeled)\":\n",
        "            st.write(\"Recommended Algorithms:\")\n",
        "            st.write(\"- **KMeans Clustering:** Centroid-based clustering for spherical clusters.\")\n",
        "            st.write(\"- **DBSCAN Clustering:** Density-based clustering for arbitrary shapes and noisy data.\")\n",
        "            st.write(\"- **Agglomerative Clustering:** Hierarchical clustering suitable for flexible group shapes.\")\n",
        "            st.write(\"- **Gaussian Mixture Models (GMM):** Probabilistic clustering assuming Gaussian distribution.\")\n",
        "\n",
        "            # Select features for clustering\n",
        "            features = st.multiselect(\"Select feature columns for clustering\", options=data.columns, default=numeric_cols)\n",
        "\n",
        "            if features:\n",
        "                st.write(\"Analyzing Dataset...\")\n",
        "                # Check for feature scaling requirement\n",
        "                scaling_required = False\n",
        "                for col in features:\n",
        "                    if data[col].max() - data[col].min() > 1000 or abs(data[col].mean()) > 1000:\n",
        "                        scaling_required = True\n",
        "                        break\n",
        "\n",
        "                if scaling_required:\n",
        "                    st.write(\"⚠️ Feature scaling is recommended for the selected columns.\")\n",
        "                    scaler = StandardScaler()\n",
        "                    data[features] = scaler.fit_transform(data[features])\n",
        "\n",
        "                # Run clustering algorithms\n",
        "                st.write(\"Running Clustering Algorithms...\")\n",
        "                start_time = time.time()\n",
        "\n",
        "                clustering_results = {}\n",
        "                silhouette_scores = {}\n",
        "\n",
        "\n",
        "                # KMeans Clustering\n",
        "                kmeans = KMeans(n_clusters=3, random_state=42).fit(data[features])\n",
        "                data['KMeans_Labels'] = kmeans.labels_\n",
        "                clustering_results['KMeans'] = kmeans.labels_\n",
        "                silhouette_scores['KMeans'] = silhouette_score(data[features], kmeans.labels_)\n",
        "\n",
        "                # DBSCAN Clustering\n",
        "                dbscan = DBSCAN().fit(data[features])\n",
        "                data['DBSCAN_Labels'] = dbscan.labels_\n",
        "                clustering_results['DBSCAN'] = dbscan.labels_\n",
        "                if len(set(dbscan.labels_)) > 1:  # Valid only if more than one cluster\n",
        "                    silhouette_scores['DBSCAN'] = silhouette_score(data[features], dbscan.labels_)\n",
        "\n",
        "                # Agglomerative Clustering\n",
        "                agglomerative = AgglomerativeClustering(n_clusters=3).fit(data[features])\n",
        "                data['Agglomerative_Labels'] = agglomerative.labels_\n",
        "                clustering_results['Agglomerative'] = agglomerative.labels_\n",
        "                silhouette_scores['Agglomerative'] = silhouette_score(data[features], agglomerative.labels_)\n",
        "\n",
        "                # Gaussian Mixture Model (GMM)\n",
        "                gmm = GaussianMixture(n_components=3, random_state=42).fit(data[features])\n",
        "                gmm_labels = gmm.predict(data[features])\n",
        "                data['GMM_Labels'] = gmm_labels\n",
        "                clustering_results['GMM'] = gmm_labels\n",
        "                silhouette_scores['GMM'] = silhouette_score(data[features], gmm_labels)\n",
        "\n",
        "                end_time = time.time()\n",
        "                st.write(f\"Clustering completed in {round(end_time - start_time, 2)} seconds\")\n",
        "\n",
        "                # Display results and recommendations\n",
        "                st.write(\"### Clustering Results and Metrics\")\n",
        "                for method, score in silhouette_scores.items():\n",
        "                    st.write(f\"**{method}:** Silhouette Score = {score:.2f}\")\n",
        "\n",
        "                # Recommend the best clustering method\n",
        "                best_method = max(silhouette_scores, key=silhouette_scores.get)\n",
        "                st.write(f\"### Recommended Clustering Method: {best_method} (Highest Silhouette Score)\")\n",
        "\n",
        "                st.write(\"coefﬁcient is between [-1, 1]. A score of 1 denotes the best, meaning that the data point i is very compact within the cluster it belongs to and far away from the other clusters. The worst value is -1. Values near 0 denote overlapping\")\n",
        "\n",
        "\n",
        "                # Visualization of clustering\n",
        "                if len(features) >= 2:\n",
        "                    st.subheader(\"Clustering Visualizations\")\n",
        "                    for method, labels in clustering_results.items():\n",
        "                        st.write(f\"**{method} Clustering:**\")\n",
        "                        fig, ax = plt.subplots()\n",
        "                        scatter = ax.scatter(data[features[0]], data[features[1]], c=labels, cmap='viridis', alpha=0.6)\n",
        "                        ax.set_xlabel(features[0])\n",
        "                        ax.set_ylabel(features[1])\n",
        "                        st.pyplot(fig)\n",
        "\n",
        "                # Download the clustering-based dataset\n",
        "                st.subheader(\"Download Clustered Dataset\")\n",
        "                csv = data.to_csv(index=False)\n",
        "                st.download_button(label=\"Download CSV\", data=csv, file_name=\"clustered_dataset.csv\", mime=\"text/csv\")\n",
        "\n",
        "                # Explain clustering methods\n",
        "                st.subheader(\"Clustering Methods Explained\")\n",
        "                st.write(\"- **KMeans Clustering:** Assigns points to clusters based on proximity to centroids. Suitable for spherical clusters.\")\n",
        "                st.write(\"- **DBSCAN Clustering:** Groups points based on density. Handles noise and arbitrary cluster shapes well.\")\n",
        "                st.write(\"- **Agglomerative Clustering:** Performs hierarchical clustering. Flexible for various data shapes.\")\n",
        "                st.write(\"- **Gaussian Mixture Models (GMM):** Assumes clusters follow Gaussian distributions.\")\n",
        "\n",
        "        else:\n",
        "            st.write(\"Supervised learning recommendations will be available on another tab.\")\n",
        "\n",
        "# Tab 5: Data Quality Insights\n",
        "with tabs[6]:\n",
        "    if selected_dataset:\n",
        "        st.header(\"Data Quality Insights\")\n",
        "\n",
        "        # Missing values\n",
        "        missing_values = data.isnull().sum()\n",
        "\n",
        "        # Outliers detection\n",
        "        outliers_count = 0\n",
        "        for col in numeric_cols:\n",
        "            # Calculate the IQR (Interquartile Range)\n",
        "            q1 = data[col].quantile(0.25)\n",
        "            q3 = data[col].quantile(0.75)\n",
        "            iqr = q3 - q1\n",
        "\n",
        "            # Find the outliers based on the IQR rule\n",
        "            outliers = ((data[col] < (q1 - 1.5 * iqr)) | (data[col] > (q3 + 1.5 * iqr)))\n",
        "            outliers_count += outliers.sum()\n",
        "\n",
        "        st.write(f\"Missing values: {missing_values.sum()} values across {len(missing_values[missing_values > 0])} columns\")\n",
        "        st.write(f\"Outliers detected in {outliers_count} data points\")\n",
        "\n",
        "        # Visualize missing values\n",
        "        st.subheader(\"Visualizing Missing Values\")\n",
        "        fig, ax = plt.subplots(figsize=(10, 5))\n",
        "        sns.heatmap(data.isnull(), cbar=False, cmap=\"viridis\", ax=ax)\n",
        "        st.pyplot(fig)\n",
        "\n",
        "        # Visualize outliers\n",
        "        st.subheader(\"Visualizing Outliers\")\n",
        "        for col in numeric_cols:\n",
        "            fig, ax = plt.subplots(figsize=(10, 5))\n",
        "            sns.boxplot(data[col], ax=ax)\n",
        "            ax.set_title(f\"Outliers in {col}\")\n",
        "            st.pyplot(fig)\n",
        "\n",
        "        # Feature Scaling Recommendation\n",
        "        st.subheader(\"Feature Scaling\")\n",
        "        st.write(\"Feature scaling recommendations for numeric columns:\")\n",
        "        scaler = StandardScaler()\n",
        "        scaled_data = pd.DataFrame(scaler.fit_transform(data[numeric_cols]), columns=numeric_cols)\n",
        "        st.write(scaled_data.describe().T)\n",
        "\n",
        "        # Encoding Recommendations\n",
        "        st.subheader(\"Encoding Recommendations\")\n",
        "        st.write(\"Ordinal and Nominal encoding recommendations for categorical columns:\")\n",
        "        for col in categorical_cols:\n",
        "            if data[col].nunique() <= 10:\n",
        "                st.write(f\"Column '{col}' can be encoded using **Ordinal Encoding**.\")\n",
        "            else:\n",
        "                st.write(f\"Column '{col}' can be encoded using **One-Hot Encoding**.\")\n",
        "\n",
        "        # Summary of Insights\n",
        "        st.subheader(\"Summary of Data Quality Insights\")\n",
        "        st.write(f\"- **Missing Values:** {missing_values.sum()} values in {missing_values[missing_values > 0].shape[0]} columns.\")\n",
        "        st.write(f\"- **Outliers:** Detected in {outliers_count} data points across numeric columns.\")\n",
        "        st.write(\"- **Feature Scaling:** Recommended for numeric columns with a wide range.\")\n",
        "        st.write(\"- **Encoding:** Recommendations provided for categorical columns.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Final Notes\n",
        "st.sidebar.info(\n",
        "    \"\"\"\n",
        "    **Instructions:**\n",
        "    - Upload your datasets in CSV, Excel, or JSON format.\n",
        "    - Navigate through the tabs to explore dataset insights, supervised/unsupervised learning recommendations, and data quality insights.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "st.sidebar.success(\"Developed by [ TC.Antony - Data Scientist] \")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqrfJpHejrwq",
        "outputId": "a8b09879-5824-462d-a56f-80835e0ed9df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app1.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Replace 'YOUR_AUTHTOKEN' with your actual ngrok authtoken\n",
        "ngrok.set_auth_token(\"2rI2XurhgC2fxlYDtteHntWpCJf_5b1kDx2SLmwgq8GukDEyc\")\n",
        "\n",
        "# Run the Streamlit app in the background\n",
        "!streamlit run app1.py &>/dev/null&\n",
        "\n",
        "# Create a public URL using ngrok\n",
        "try:\n",
        "    public_url = ngrok.connect(8501)\n",
        "    print(f\"Streamlit app is running at {public_url}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"Trying to run with localtunnel\")\n",
        "    !streamlit run app1.py &>/content/logs.txt & npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDO7Arv4QW7h",
        "outputId": "10d4a8a4-05d6-44f1-b8f9-4dddef8d3864"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit app is running at NgrokTunnel: \"https://5388-34-106-125-219.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "noQ4FZsabyAz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}